{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RadFact Example\n",
    "\n",
    "Here we show an example of running RadFact for evaluation of either findings generation or grounded reporting evaluation.\n",
    "\n",
    "## Endpoint setup\n",
    "\n",
    "RadFact scores in the MAIRA-2 paper are computed using `Llama-3-70b-Instruct` for entailment verification and GPT-4 for report to phrase conversion.\n",
    "\n",
    "* Edit [`configs/endpoints/azure_chat_openai.yaml`](configs/endpoints/azure_chat_openai.yaml) to configure the endpoints for the Azure Chat API. This will be used by default for parsing the reports into phrases.\n",
    "* Edit [`configs/endpoints/chat_openai.yaml`](configs/endpoints/chat_openai.yaml) to configure the endpoints for the Chat API. This will be used by default for entailement verification. \n",
    "* Set env variable `API_KEY` if you want to use key-based authentication for these endpoints. In case you're using multiple endpoints, use different env variables for each endpoint, e.g., `API_KEY_CHAT_OPENAI` and `API_KEY_AZURE_CHAT_OPENAI`. Make sure to update the corresponding endpoint config files to use these env variables names in `api_key_env_var_name`.\n",
    "* Update `endpoints` in [`configs/radfact.yaml`](configs/radfact.yaml) and [`configs/report_to_phrases.yaml`](src/report_to_phrases.yaml) to use either `ChatOpenAI` or `AzureChatOpenAI` endpoints as available.\n",
    "\n",
    "See the [README](README.md#2-endpoint-llm-setup) for more detailed setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from radfact.data_utils.grounded_phrase_list import GroundedPhraseList\n",
    "from radfact.metric.radfact import RadFactMetric\n",
    "from radfact.metric.bootstrapping import MetricBootstrapper\n",
    "from radfact.metric.print_utils import print_bootstrap_results, print_results\n",
    "from radfact.paths import EXAMPLES_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings Generation Evaluation\n",
    "\n",
    "We provide an example csv in [`findings_generation_examples.csv`](examples/findings_generation_examples.csv) with columns `example_id`, `prediction` (model generation), `target` (ground truth).\n",
    "\n",
    "RadFact expects `candidates` (generations) and `references` (ground truths) in a dictionary where keys are an identifier, typically the study id. We use `example_id` here. `candidates` and `references` are expected to be strings corresponding to the predicted and target findings sections. They will first get converted into phrases using the report to phrase conversion prompts and then undergo entailment verification to get RadFact scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_generation_examples = pd.read_csv(EXAMPLES_DIR / 'findings_generation_examples.csv')\n",
    "display(findings_generation_examples.head(2))\n",
    "candidates_fg = findings_generation_examples.set_index(\"example_id\")[\"prediction\"].to_dict()\n",
    "references_fg = findings_generation_examples.set_index(\"example_id\")[\"target\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For findings generation, when we initialise the metric we set `is_narrative_text=True` to instruct it to first perfom report-to-phrase conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radfact_metric_for_fg = RadFactMetric(is_narrative_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logical_f1_fg` and `radfact_scores_f` can directly be obtained using the [`compute_metric_score`](radfact/src/radfact/metric/radfact.py#L369) method as shown below.\n",
    "\n",
    "```python \n",
    "logical_f1_fg, radfact_scores_f = radfact_metric_for_fg.compute_metric_score(candidates, references)\n",
    "```\n",
    "This calls [`compute_results_per_sample`](radfact/src/radfact/metric/radfact.py#L284) and [`aggregate_results`](radfact/src/radfact/metric/radfact.py#L355) under the hood. However, we break it down explicitely in this example to be able to reuse the per sample results for bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_sample_fg = radfact_metric_for_fg.compute_results_per_sample(candidates_fg, references_fg)\n",
    "logical_f1_fg, radfact_scores_fg = radfact_metric_for_fg.aggregate_results(results_per_sample_fg)\n",
    "logical_f1_fg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the results. The only relevant scores for finding generation are logical_precision, logical_recall and logical_f1 since there are no boxes associated with findings to compute the other grounding and spatial scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Findings generation RadFact scores:\")\n",
    "print_results(radfact_scores_fg, metrics=[\"logical_precision\", \"logical_recall\", \"logical_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compute the bootstrap confidence intervals for the scores as shown below.\n",
    "\n",
    "We set the number of bootstrap samples (`num_samples`) to 10 here because our example dataset is quite small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapper = MetricBootstrapper(metric=radfact_metric_for_fg, num_samples=10, seed=42)\n",
    "radfact_scores_fg_with_cis = bootstrapper.compute_bootstrap_metrics(results_per_sample=results_per_sample_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the results with the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Findings generation RadFact scores (95% CI):\")\n",
    "print_bootstrap_results(radfact_scores_fg_with_cis, metrics=[\"logical_precision\", \"logical_recall\", \"logical_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounded Reporting Evaluation\n",
    "\n",
    "For grounded reporting, it's easiest to store model generations and ground truth in JSON format to accommodate both text and boxes. Each grounded report is represented as a list of dicts representing individual sentences, each with `text` and `boxes` keys. The `boxes` are `None` for non-grounded sentences. As for findings generation, the model generations are under `prediction` and the ground truth is under `target`.\n",
    "\n",
    "Refer to the [grounded_reporting_examples.json](examples/grounded_reporting_examples.json) for examples of the expected JSON format.\n",
    "\n",
    "From this JSON we can parse examples easily into `GroundedPhraseList`, which is expected by RadFact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXAMPLES_DIR / 'grounded_reporting_examples.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    grounded_reporting_examples = json.load(f)\n",
    "candidates_gr = {\n",
    "    example[\"example_id\"]: GroundedPhraseList.from_list_of_dicts(example[\"prediction\"])\n",
    "    for example in grounded_reporting_examples\n",
    "}\n",
    "references_gr = {\n",
    "    example[\"example_id\"]: GroundedPhraseList.from_list_of_dicts(example[\"target\"])\n",
    "    for example in grounded_reporting_examples\n",
    "}\n",
    "print(\"Loaded\", len(grounded_reporting_examples), \"grounded reporting examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When operating on grounded reports, represented as `GroundedPhraseList`, we do not need to set `is_narrative_text=True` in the metric. With already-parsed reports, no step to convert reports into phrases is required. `is_narrative_text` is set to `False` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radfact_metric_for_gr = RadFactMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to findings generation, we can compute the metric scores and confidence intervals for grounded reporting.\n",
    "\n",
    "We also break down the computation to be able to reuse the per sample results for bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_sample_gr = radfact_metric_for_gr.compute_results_per_sample(candidates_gr, references_gr)\n",
    "logical_f1_gr, radfact_scores_gr = radfact_metric_for_gr.aggregate_results(results_per_sample_gr)\n",
    "logical_f1_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is grounded reporting, we look at all the metrics returned by RadFact including grounding and spatial scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"logical_precision\",\n",
    "    \"logical_recall\",\n",
    "    \"logical_f1\",\n",
    "    \"spatial_precision\",\n",
    "    \"spatial_recall\",\n",
    "    \"spatial_f1\",\n",
    "    \"grounding_precision\",\n",
    "    \"grounding_recall\",\n",
    "    \"grounding_f1\",\n",
    "]\n",
    "print(\"Grounded reporting RadFact scores:\")\n",
    "print_results(radfact_scores_gr, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the bootstrap confidence intervals for the scores similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapper = MetricBootstrapper(metric=radfact_metric_for_gr, num_samples=10, seed=42)\n",
    "radfact_scores_gr_with_cis = bootstrapper.compute_bootstrap_metrics(results_per_sample=results_per_sample_gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the metrics with the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grounded reporting RadFact scores (95% CI):\")\n",
    "print_bootstrap_results(radfact_scores_gr_with_cis, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radfact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
